#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VAEå­¦ç¿’ãƒ†ã‚¹ãƒˆ - ã‚ªãƒ¼ãƒ«ã‚¤ãƒ³ãƒ¯ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ã„æ–¹:
    python test_vae_training.py

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:
1. ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
2. ãƒ¢ãƒ‡ãƒ«ä½œæˆ
3. å°è¦æ¨¡å­¦ç¿’ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ï¼‰
4. çµæœç¢ºèª
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt


# ============================================================
# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
# ============================================================

def get_device():
    """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—"""
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        print(f"âœ“ Using CUDA: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ“ Using MPS (Apple Silicon)")
    else:
        device = torch.device("cpu")
        print("âœ“ Using CPU")
    return device


# ============================================================
# ãƒ¢ãƒ‡ãƒ«å®šç¾©
# ============================================================

class SkeletonEncoder(nn.Module):
    """éª¨æ ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼"""
    def __init__(self, z_dim=128):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),
            nn.Conv2d(256, 512, 4, 2, 1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),
        )
        self.fc_mean = nn.Linear(512 * 8 * 8, z_dim)
        self.fc_logvar = nn.Linear(512 * 8 * 8, z_dim)

    def forward(self, x):
        h = self.conv(x).view(x.size(0), -1)
        return self.fc_mean(h), self.fc_logvar(h)


class StyleEncoder(nn.Module):
    """ã‚¹ã‚¿ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼"""
    def __init__(self, z_dim=64):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),
        )
        self.pool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc_mean = nn.Linear(256 * 16, z_dim)
        self.fc_logvar = nn.Linear(256 * 16, z_dim)

    def forward(self, x):
        h = self.pool(self.conv(x)).view(x.size(0), -1)
        return self.fc_mean(h), self.fc_logvar(h)


class FontDecoder(nn.Module):
    """ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼"""
    def __init__(self, z_content_dim=128, z_style_dim=64):
        super().__init__()
        self.fc = nn.Linear(z_content_dim + z_style_dim, 512 * 8 * 8)
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),
            nn.ConvTranspose2d(64, 1, 4, 2, 1), nn.Sigmoid(),
        )

    def forward(self, z_content, z_style):
        z = torch.cat([z_content, z_style], dim=1)
        h = self.fc(z).view(-1, 512, 8, 8)
        return self.deconv(h)


class FontVAE(nn.Module):
    """ãƒ•ã‚©ãƒ³ãƒˆVAE"""
    def __init__(self, z_content_dim=128, z_style_dim=64):
        super().__init__()
        self.skeleton_encoder = SkeletonEncoder(z_content_dim)
        self.style_encoder = StyleEncoder(z_style_dim)
        self.decoder = FontDecoder(z_content_dim, z_style_dim)

    def reparameterize(self, mean, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mean + eps * std

    def forward(self, x):
        mean_content, logvar_content = self.skeleton_encoder(x)
        mean_style, logvar_style = self.style_encoder(x)
        z_content = self.reparameterize(mean_content, logvar_content)
        z_style = self.reparameterize(mean_style, logvar_style)
        reconstructed = self.decoder(z_content, z_style)
        return {
            'reconstructed': reconstructed,
            'mean_content': mean_content,
            'logvar_content': logvar_content,
            'mean_style': mean_style,
            'logvar_style': logvar_style,
        }


# ============================================================
# æå¤±é–¢æ•°
# ============================================================

def vae_loss(outputs, original):
    """VAEæå¤±"""
    reconstructed = outputs['reconstructed']
    mean_content = outputs['mean_content']
    logvar_content = outputs['logvar_content']
    mean_style = outputs['mean_style']
    logvar_style = outputs['logvar_style']

    # å†æ§‹æˆæå¤±
    recon_loss = F.mse_loss(reconstructed, original) + F.binary_cross_entropy(reconstructed, original)

    # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
    kl_content = -0.5 * torch.sum(1 + logvar_content - mean_content.pow(2) - logvar_content.exp())
    kl_style = -0.5 * torch.sum(1 + logvar_style - mean_style.pow(2) - logvar_style.exp())
    kl_loss = (kl_content + kl_style) / original.size(0)

    total_loss = recon_loss + 0.001 * kl_loss

    return total_loss, recon_loss, kl_loss


# ============================================================
# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# ============================================================

class DummyFontDataset(Dataset):
    """ãƒ€ãƒŸãƒ¼ãƒ•ã‚©ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    def __init__(self, num_samples=1000, image_size=128):
        self.num_samples = num_samples
        self.image_size = image_size

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡å­—é¢¨ã®ç”»åƒã‚’ç”Ÿæˆ
        img = np.zeros((self.image_size, self.image_size), dtype=np.float32)

        # ãƒ©ãƒ³ãƒ€ãƒ ãªç·šã‚’æç”»
        for _ in range(np.random.randint(3, 8)):
            x1, y1 = np.random.randint(20, self.image_size - 20, 2)
            x2, y2 = np.random.randint(20, self.image_size - 20, 2)
            thickness = np.random.randint(2, 6)

            # ç·šã‚’æç”»ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            pts = np.linspace([x1, y1], [x2, y2], 100).astype(int)
            for px, py in pts:
                if 0 <= px < self.image_size and 0 <= py < self.image_size:
                    img[py, px] = 1.0
                    for dx in range(-thickness, thickness):
                        for dy in range(-thickness, thickness):
                            nx, ny = px + dx, py + dy
                            if 0 <= nx < self.image_size and 0 <= ny < self.image_size:
                                img[ny, nx] = 1.0

        return torch.from_numpy(img).unsqueeze(0)  # (1, H, W)


# ============================================================
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹
# ============================================================

def calculate_ssim_simple(pred, target):
    """ç°¡æ˜“SSIMè¨ˆç®—"""
    C1, C2 = 0.01 ** 2, 0.03 ** 2
    mu1, mu2 = pred.mean(), target.mean()
    sigma1, sigma2 = pred.std(), target.std()
    sigma12 = ((pred - mu1) * (target - mu2)).mean()
    ssim = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / \
           ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1 ** 2 + sigma2 ** 2 + C2))
    return ssim.item()


# ============================================================
# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
# ============================================================

def train():
    """å­¦ç¿’å®Ÿè¡Œ"""
    print("=" * 60)
    print(" VAEå­¦ç¿’ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)

    # ãƒ‡ãƒã‚¤ã‚¹
    device = get_device()

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    print("\nğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
    train_dataset = DummyFontDataset(num_samples=500)
    val_dataset = DummyFontDataset(num_samples=100)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    print(f"  Train: {len(train_dataset)} samples")
    print(f"  Val: {len(val_dataset)} samples")

    # ãƒ¢ãƒ‡ãƒ«
    print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
    model = FontVAE(z_content_dim=128, z_style_dim=64).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"  Total parameters: {total_params:,}")

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # å­¦ç¿’
    print("\nğŸ“ å­¦ç¿’é–‹å§‹...")
    num_epochs = 5
    best_val_loss = float('inf')

    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # Train
        model.train()
        epoch_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs}")

        for batch in pbar:
            images = batch.to(device)

            outputs = model(images)
            total_loss, recon_loss, kl_loss = vae_loss(outputs, images)

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

            epoch_loss += total_loss.item()

            pbar.set_postfix({'loss': total_loss.item()})

        train_loss = epoch_loss / len(train_loader)
        train_losses.append(train_loss)

        # Validation
        model.eval()
        val_loss_total = 0.0
        ssim_total = 0.0

        with torch.no_grad():
            for batch in val_loader:
                images = batch.to(device)
                outputs = model(images)
                total_loss, _, _ = vae_loss(outputs, images)
                val_loss_total += total_loss.item()

                # SSIM
                ssim = calculate_ssim_simple(outputs['reconstructed'], images)
                ssim_total += ssim

        val_loss = val_loss_total / len(val_loader)
        val_ssim = ssim_total / len(val_loader)
        val_losses.append(val_loss)

        is_best = val_loss < best_val_loss
        if is_best:
            best_val_loss = val_loss

        print(f"\n  Train Loss: {train_loss:.4f}")
        print(f"  Val Loss: {val_loss:.4f}, SSIM: {val_ssim:.4f} {'âœ“ Best!' if is_best else ''}")

    print("\nâœ… å­¦ç¿’å®Œäº†!")
    print(f"  Best Val Loss: {best_val_loss:.4f}")

    # å¯è¦–åŒ–
    print("\nğŸ“Š çµæœå¯è¦–åŒ–ä¸­...")

    # å­¦ç¿’æ›²ç·š
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Curve')
    plt.legend()
    plt.grid(True)

    # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒ
    model.eval()
    with torch.no_grad():
        sample_batch = next(iter(val_loader)).to(device)[:8]
        outputs = model(sample_batch)
        reconstructed = outputs['reconstructed'][:8]

    plt.subplot(1, 2, 2)
    comparison = torch.cat([sample_batch.cpu(), reconstructed.cpu()], dim=0)
    grid = comparison.view(-1, 1, 128, 128).permute(0, 2, 3, 1).squeeze().numpy()

    # 8x2ã‚°ãƒªãƒƒãƒ‰
    display_grid = np.zeros((128 * 2, 128 * 8))
    for i in range(8):
        display_grid[0:128, i * 128:(i + 1) * 128] = grid[i]
        display_grid[128:256, i * 128:(i + 1) * 128] = grid[i + 8]

    plt.imshow(display_grid, cmap='gray')
    plt.title('Original (top) vs Reconstructed (bottom)')
    plt.axis('off')

    plt.tight_layout()
    plt.savefig('vae_test_results.png', dpi=150, bbox_inches='tight')
    print("  ä¿å­˜: vae_test_results.png")

    print("\n" + "=" * 60)
    print(" ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    print("=" * 60)
    print("\nâœ“ ãƒ¢ãƒ‡ãƒ«ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
    print("âœ“ å­¦ç¿’ãŒåæŸã—ã¦ã„ã¾ã™")
    print(f"âœ“ SSIM: {val_ssim:.3f} (0.7ä»¥ä¸ŠãŒç›®æ¨™)")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. å®Ÿéš›ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’")
    print("  2. ã‚ˆã‚Šé•·æ™‚é–“ï¼ˆ50-200ã‚¨ãƒãƒƒã‚¯ï¼‰å­¦ç¿’")
    print("  3. ç”Ÿæˆå“è³ªã‚’è©•ä¾¡")


if __name__ == "__main__":
    train()
